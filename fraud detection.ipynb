{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc21f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch_geometric\n",
    "#!pip install ipywidgets\n",
    "#!pip install --upgrade torch\n",
    "#!pip3 install dgl\n",
    "#!pip install tf-keras\n",
    "#!pip install pytorch-tabnet\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cde91f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip_address</th>\n",
       "      <th>click_time</th>\n",
       "      <th>device_type</th>\n",
       "      <th>os_version</th>\n",
       "      <th>browser</th>\n",
       "      <th>site_id</th>\n",
       "      <th>ad_id</th>\n",
       "      <th>click_count</th>\n",
       "      <th>time_on_site</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102</td>\n",
       "      <td>37892</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>206</td>\n",
       "      <td>354</td>\n",
       "      <td>4</td>\n",
       "      <td>99.039870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>435</td>\n",
       "      <td>1015</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>335</td>\n",
       "      <td>287</td>\n",
       "      <td>4</td>\n",
       "      <td>7.977968</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>860</td>\n",
       "      <td>61813</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>592</td>\n",
       "      <td>163</td>\n",
       "      <td>7</td>\n",
       "      <td>211.605775</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>270</td>\n",
       "      <td>27712</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>729</td>\n",
       "      <td>140</td>\n",
       "      <td>9</td>\n",
       "      <td>48.571644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106</td>\n",
       "      <td>8415</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>513</td>\n",
       "      <td>453</td>\n",
       "      <td>3</td>\n",
       "      <td>12.121390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>71</td>\n",
       "      <td>62292</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>313</td>\n",
       "      <td>4</td>\n",
       "      <td>8.439609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>700</td>\n",
       "      <td>23833</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>395</td>\n",
       "      <td>127</td>\n",
       "      <td>3</td>\n",
       "      <td>53.717202</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>4158</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>370</td>\n",
       "      <td>397</td>\n",
       "      <td>4</td>\n",
       "      <td>60.143515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>614</td>\n",
       "      <td>62680</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>857</td>\n",
       "      <td>420</td>\n",
       "      <td>26</td>\n",
       "      <td>2.649432</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>121</td>\n",
       "      <td>20309</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>790</td>\n",
       "      <td>367</td>\n",
       "      <td>28</td>\n",
       "      <td>3.291645</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ip_address  click_time  device_type  os_version  browser  site_id  ad_id  \\\n",
       "0         102       37892            1           1        3      206    354   \n",
       "1         435        1015            2           5        6      335    287   \n",
       "2         860       61813            4           8        1      592    163   \n",
       "3         270       27712            0           1        2      729    140   \n",
       "4         106        8415            4           9        2      513    453   \n",
       "5          71       62292            1           1        0      177    313   \n",
       "6         700       23833            1           2        0      395    127   \n",
       "7          20        4158            2           6        4      370    397   \n",
       "8         614       62680            4           5        1      857    420   \n",
       "9         121       20309            0           7        0      790    367   \n",
       "\n",
       "   click_count  time_on_site  fraud  \n",
       "0            4     99.039870      0  \n",
       "1            4      7.977968      0  \n",
       "2            7    211.605775      0  \n",
       "3            9     48.571644      0  \n",
       "4            3     12.121390      0  \n",
       "5            4      8.439609      0  \n",
       "6            3     53.717202      0  \n",
       "7            4     60.143515      0  \n",
       "8           26      2.649432      1  \n",
       "9           28      3.291645      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples =1000\n",
    "\n",
    "# Features\n",
    "ip_addresses = np.random.randint(0, 1000, n_samples)\n",
    "click_time = np.random.randint(0, 24*60*60, n_samples)\n",
    "device_type = np.random.randint(0, 5, n_samples)\n",
    "os_version = np.random.randint(0, 10, n_samples)\n",
    "browser = np.random.randint(0, 8, n_samples)\n",
    "site_id = np.random.randint(0, 1000, n_samples)\n",
    "ad_id = np.random.randint(0, 500, n_samples)\n",
    "click_count = np.random.poisson(5, n_samples)\n",
    "time_on_site = np.random.exponential(60, n_samples)\n",
    "\n",
    "# Create fraud labels (1 for fraud, 0 for non-fraud)\n",
    "fraud = np.random.choice([0, 1], n_samples, p=[0.6, 0.4])  # 20% fraud rate\n",
    "\n",
    "# Introduce some patterns for fraudulent behavior\n",
    "fraud_mask = fraud == 1\n",
    "click_count[fraud_mask] += np.random.poisson(20, sum(fraud_mask))\n",
    "time_on_site[fraud_mask] = np.random.uniform(0, 5, sum(fraud_mask))\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'ip_address': ip_addresses,\n",
    "    'click_time': click_time,\n",
    "    'device_type': device_type,\n",
    "    'os_version': os_version,\n",
    "    'browser': browser,\n",
    "    'site_id': site_id,\n",
    "    'ad_id': ad_id,\n",
    "    'click_count': click_count,\n",
    "    'time_on_site': time_on_site,\n",
    "    'fraud': fraud\n",
    "})\n",
    "\n",
    "# Split the data\n",
    "X = data.drop('fraud', axis=1)\n",
    "y = data['fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(len(data))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a018bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "Accuracy: 0.9900\n",
      "Precision: 0.9881\n",
      "Recall: 0.9881\n",
      "F1-score: 0.9881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faaad332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results:\n",
      "Accuracy: 0.9900\n",
      "Precision: 0.9881\n",
      "Recall: 0.9881\n",
      "F1-score: 0.9881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = rf_clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc2e6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting (XGBoost) Results:\n",
      "Accuracy: 0.9900\n",
      "Precision: 0.9881\n",
      "Recall: 0.9881\n",
      "F1-score: 0.9881\n",
      "\n",
      "Top 5 most important features:\n",
      "f7: 99.77572631835938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_scaled, label=y_test)\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "num_round = 100\n",
    "model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(dtest)\n",
    "y_pred_binary = [1 if y > 0.5 else 0 for y in y_pred]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "print(\"Gradient Boosting (XGBoost) Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "importance = model.get_score(importance_type='gain')\n",
    "sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 5 most important features:\")\n",
    "for feature, score in sorted_importance[:5]:\n",
    "    print(f\"{feature}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae9b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Results:\n",
      "Accuracy: 0.9900\n",
      "Precision: 0.9881\n",
      "Recall: 0.9881\n",
      "F1-score: 0.9881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create and train the SVM model\n",
    "svm_model = SVC(kernel='rbf', C=1.0, random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Support Vector Machine Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e75c4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GCN Results:\n",
      "Accuracy: 0.8550\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Create graph structure\n",
    "edge_index = []\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(i+1, len(X_train)):\n",
    "        if X_train.iloc[i]['ip_address'] == X_train.iloc[j]['ip_address']:\n",
    "            edge_index.append([i, j])\n",
    "            edge_index.append([j, i])\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "x = torch.tensor(X_train_scaled, dtype=torch.float)\n",
    "y = torch.tensor(y_train.values, dtype=torch.long)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "class GCNModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCNModel(num_features=X_train.shape[1], hidden_channels=64, num_classes=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    acc = (pred == data.y).sum().item() / data.y.size(0)\n",
    "\n",
    "print(\"\\nGCN Results:\")\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "115a3911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: torch.Size([800, 9])\n",
      "Original label shape: torch.Size([800])\n",
      "Number of nodes: 800\n",
      "Number of edges: 668\n",
      "Shape of data.x: torch.Size([800, 9])\n",
      "Shape of data.y: torch.Size([800])\n",
      "Epoch: 010, Loss: 0.9336, Train Acc: 0.8359, Test Acc: 0.7875\n",
      "Epoch: 020, Loss: 0.8185, Train Acc: 0.8625, Test Acc: 0.8313\n",
      "Epoch: 030, Loss: 0.6810, Train Acc: 0.8672, Test Acc: 0.8250\n",
      "Epoch: 040, Loss: 0.6672, Train Acc: 0.8641, Test Acc: 0.8313\n",
      "Epoch: 050, Loss: 0.6153, Train Acc: 0.8594, Test Acc: 0.8250\n",
      "Epoch: 060, Loss: 0.6162, Train Acc: 0.8547, Test Acc: 0.8125\n",
      "Epoch: 070, Loss: 0.6069, Train Acc: 0.8578, Test Acc: 0.8187\n",
      "Epoch: 080, Loss: 0.6032, Train Acc: 0.8625, Test Acc: 0.8250\n",
      "Epoch: 090, Loss: 0.5940, Train Acc: 0.8609, Test Acc: 0.8250\n",
      "Epoch: 100, Loss: 0.6329, Train Acc: 0.8625, Test Acc: 0.8250\n",
      "Epoch: 110, Loss: 0.6000, Train Acc: 0.8625, Test Acc: 0.8250\n",
      "Epoch: 120, Loss: 0.6007, Train Acc: 0.8625, Test Acc: 0.8250\n",
      "Epoch: 130, Loss: 0.5899, Train Acc: 0.8609, Test Acc: 0.8250\n",
      "Epoch: 140, Loss: 0.6050, Train Acc: 0.8625, Test Acc: 0.8250\n",
      "Epoch: 150, Loss: 0.6020, Train Acc: 0.8625, Test Acc: 0.8250\n",
      "Epoch: 160, Loss: 0.5975, Train Acc: 0.8609, Test Acc: 0.8250\n",
      "Epoch: 170, Loss: 0.6090, Train Acc: 0.8625, Test Acc: 0.8250\n",
      "Epoch: 180, Loss: 0.6069, Train Acc: 0.8594, Test Acc: 0.8250\n",
      "Epoch: 190, Loss: 0.6294, Train Acc: 0.8625, Test Acc: 0.8250\n",
      "Epoch: 200, Loss: 0.5974, Train Acc: 0.8625, Test Acc: 0.8250\n",
      "Final Train Accuracy: 0.8625\n",
      "Final Test Accuracy: 0.8250\n",
      "\n",
      "Top 5 most important features:\n",
      "Feature 7: 0.2191\n",
      "Feature 8: 0.1483\n",
      "Feature 3: 0.0862\n",
      "Feature 1: 0.0811\n",
      "Feature 6: 0.0810\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'data' is already a PyTorch Geometric Data object\n",
    "print(\"Original data shape:\", data.x.shape)\n",
    "print(\"Original label shape:\", data.y.shape)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "data.x = torch.tensor(scaler.fit_transform(data.x), dtype=torch.float)\n",
    "\n",
    "# Split the data\n",
    "num_nodes = data.num_nodes\n",
    "indices = torch.randperm(num_nodes)\n",
    "train_indices = indices[:int(0.8 * num_nodes)]\n",
    "test_indices = indices[int(0.8 * num_nodes):]\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "train_mask[train_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "print(\"Number of nodes:\", data.num_nodes)\n",
    "print(\"Number of edges:\", data.num_edges)\n",
    "print(\"Shape of data.x:\", data.x.shape)\n",
    "print(\"Shape of data.y:\", data.y.shape)\n",
    "\n",
    "# Define the GAT model\n",
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=8, dropout=0.6)\n",
    "        self.conv2 = GATConv(hidden_channels * 8, out_channels, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "# Initialize the model\n",
    "model = GATModel(in_channels=data.num_features, hidden_channels=8, out_channels=2)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Testing function\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=-1)\n",
    "        train_correct = pred[data.train_mask] == data.y[data.train_mask]\n",
    "        train_acc = int(train_correct.sum()) / int(data.train_mask.sum())\n",
    "        test_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "        test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "    return train_acc, test_acc\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    if epoch % 10 == 0:\n",
    "        train_acc, test_acc = test()\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "# Final evaluation\n",
    "train_acc, test_acc = test()\n",
    "print(f'Final Train Accuracy: {train_acc:.4f}')\n",
    "print(f'Final Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Interpret the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get the weights of the first convolutional layer\n",
    "    conv1_weights = model.conv1.lin.weight.abs().mean(dim=0)\n",
    "\n",
    "# Get the top 5 most important features based on the weights\n",
    "feature_importance = conv1_weights.cpu().numpy()\n",
    "top_features = feature_importance.argsort()[::-1][:5]\n",
    "\n",
    "print(\"\\nTop 5 most important features:\")\n",
    "for idx in top_features:\n",
    "    print(f\"Feature {idx}: {feature_importance[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5313306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data.x: torch.Size([800, 9])\n",
      "Shape of data.y: torch.Size([800])\n",
      "Graph data:\n",
      "Number of nodes: 800\n",
      "Number of edges: 668\n",
      "Shape of data.x: torch.Size([800, 9])\n",
      "Shape of data.y: torch.Size([800])\n",
      "Epoch: 010, Loss: 0.2497, Train Acc: 0.9771, Val Acc: 0.9750, Test Acc: 0.9812\n",
      "Epoch: 020, Loss: 0.0886, Train Acc: 0.9979, Val Acc: 0.9938, Test Acc: 1.0000\n",
      "Epoch: 030, Loss: 0.0236, Train Acc: 0.9979, Val Acc: 0.9938, Test Acc: 1.0000\n",
      "Epoch: 040, Loss: 0.0138, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 050, Loss: 0.0115, Train Acc: 1.0000, Val Acc: 0.9938, Test Acc: 1.0000\n",
      "Epoch: 060, Loss: 0.0089, Train Acc: 1.0000, Val Acc: 0.9938, Test Acc: 1.0000\n",
      "Epoch: 070, Loss: 0.0065, Train Acc: 1.0000, Val Acc: 0.9938, Test Acc: 1.0000\n",
      "Epoch: 080, Loss: 0.0039, Train Acc: 1.0000, Val Acc: 0.9938, Test Acc: 1.0000\n",
      "Epoch: 090, Loss: 0.0037, Train Acc: 1.0000, Val Acc: 0.9938, Test Acc: 1.0000\n",
      "Epoch: 100, Loss: 0.0054, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 110, Loss: 0.0027, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 120, Loss: 0.0032, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 130, Loss: 0.0036, Train Acc: 1.0000, Val Acc: 0.9938, Test Acc: 1.0000\n",
      "Epoch: 140, Loss: 0.0020, Train Acc: 1.0000, Val Acc: 0.9938, Test Acc: 1.0000\n",
      "Epoch: 150, Loss: 0.0036, Train Acc: 1.0000, Val Acc: 0.9938, Test Acc: 1.0000\n",
      "Epoch: 160, Loss: 0.0023, Train Acc: 1.0000, Val Acc: 0.9938, Test Acc: 1.0000\n",
      "Epoch: 170, Loss: 0.0020, Train Acc: 1.0000, Val Acc: 0.9938, Test Acc: 1.0000\n",
      "Epoch: 180, Loss: 0.0017, Train Acc: 1.0000, Val Acc: 0.9938, Test Acc: 1.0000\n",
      "Epoch: 190, Loss: 0.0013, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Epoch: 200, Loss: 0.0017, Train Acc: 1.0000, Val Acc: 1.0000, Test Acc: 1.0000\n",
      "Final Train Accuracy: 1.0000\n",
      "Final Validation Accuracy: 1.0000\n",
      "Final Test Accuracy: 1.0000\n",
      "\n",
      "Top 5 most important features:\n",
      "Feature 7: 0.9099\n",
      "Feature 8: 0.6440\n",
      "Feature 4: 0.1714\n",
      "Feature 0: 0.1414\n",
      "Feature 2: 0.1085\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Shape of data.x:\", data.x.shape)\n",
    "print(\"Shape of data.y:\", data.y.shape)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "data.x = torch.tensor(scaler.fit_transform(data.x.numpy()), dtype=torch.float)\n",
    "\n",
    "# Split the data\n",
    "num_nodes = data.num_nodes\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_indices = torch.randperm(num_nodes)[:int(0.6 * num_nodes)]\n",
    "val_indices = torch.randperm(num_nodes)[int(0.6 * num_nodes):int(0.8 * num_nodes)]\n",
    "test_indices = torch.randperm(num_nodes)[int(0.8 * num_nodes):]\n",
    "\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "print(\"Graph data:\")\n",
    "print(\"Number of nodes:\", data.num_nodes)\n",
    "print(\"Number of edges:\", data.num_edges)\n",
    "print(\"Shape of data.x:\", data.x.shape)\n",
    "print(\"Shape of data.y:\", data.y.shape)\n",
    "\n",
    "# Define the GraphSAGE model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize the model\n",
    "model = GraphSAGE(in_channels=data.num_features, hidden_channels=64, out_channels=2)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Testing function\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        train_correct = pred[data.train_mask] == data.y[data.train_mask]\n",
    "        val_correct = pred[data.val_mask] == data.y[data.val_mask]\n",
    "        test_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "        train_acc = int(train_correct.sum()) / int(data.train_mask.sum())\n",
    "        val_acc = int(val_correct.sum()) / int(data.val_mask.sum())\n",
    "        test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "    return train_acc, val_acc, test_acc\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    if epoch % 10 == 0:\n",
    "        train_acc, val_acc, test_acc = test()\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "# Final evaluation\n",
    "train_acc, val_acc, test_acc = test()\n",
    "print(f'Final Train Accuracy: {train_acc:.4f}')\n",
    "print(f'Final Validation Accuracy: {val_acc:.4f}')\n",
    "print(f'Final Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Interpret the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    node_embeddings = model.conv1(data.x, data.edge_index)\n",
    "\n",
    "# Calculate feature importance based on the correlation between input features and learned embeddings\n",
    "feature_importance = torch.zeros(data.num_features)\n",
    "for i in range(data.num_features):\n",
    "    correlation = torch.corrcoef(torch.stack([data.x[:, i], node_embeddings.mean(dim=1)]))\n",
    "    feature_importance[i] = abs(correlation[0, 1])\n",
    "\n",
    "# Get the top 5 most important features\n",
    "top_features = feature_importance.argsort(descending=True)[:5]\n",
    "\n",
    "print(\"\\nTop 5 most important features:\")\n",
    "for idx in top_features:\n",
    "    print(f\"Feature {idx.item()}: {feature_importance[idx].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "854d3553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1000, 9)\n",
      "Shape of y: (1000,)\n",
      "Graph created successfully\n",
      "Number of nodes: 1000\n",
      "Number of edges: 1092\n",
      "Shape of graph_data.x: torch.Size([1000, 9])\n",
      "Shape of graph_data.y: torch.Size([1000])\n",
      "Epoch: 010, Loss: 0.0009\n",
      "Epoch: 020, Loss: 0.0001\n",
      "Epoch: 030, Loss: 0.0000\n",
      "Epoch: 040, Loss: 0.0000\n",
      "Epoch: 050, Loss: 0.0000\n",
      "Epoch: 060, Loss: 0.0000\n",
      "Epoch: 070, Loss: 0.0000\n",
      "Epoch: 080, Loss: 0.0000\n",
      "Epoch: 090, Loss: 0.0000\n",
      "Epoch: 100, Loss: 0.0000\n",
      "Epoch: 110, Loss: 0.0000\n",
      "Epoch: 120, Loss: 0.0000\n",
      "Epoch: 130, Loss: 0.0000\n",
      "Epoch: 140, Loss: 0.0000\n",
      "Epoch: 150, Loss: 0.0000\n",
      "Epoch: 160, Loss: 0.0000\n",
      "Epoch: 170, Loss: 0.0000\n",
      "Epoch: 180, Loss: 0.0000\n",
      "Epoch: 190, Loss: 0.0000\n",
      "Epoch: 200, Loss: 0.0000\n",
      "\n",
      "Accuracy: 0.5870\n",
      "\n",
      "Top 5 most important features:\n",
      "site_id: 0.2947\n",
      "os_version: 0.2873\n",
      "browser: 0.2822\n",
      "ad_id: 0.2806\n",
      "ip_address: 0.2778\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, InnerProductDecoder\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'ip_address': ip_addresses,\n",
    "    'click_time': click_time,\n",
    "    'device_type': device_type,\n",
    "    'os_version': os_version,\n",
    "    'browser': browser,\n",
    "    'site_id': site_id,\n",
    "    'ad_id': ad_id,\n",
    "    'click_count': click_count,\n",
    "    'time_on_site': time_on_site,\n",
    "    'fraud': fraud\n",
    "})\n",
    "\n",
    "# Split the data\n",
    "X = data.drop('fraud', axis=1)\n",
    "y = data['fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Create graph structure\n",
    "def create_graph(X, y):\n",
    "    edge_index = []\n",
    "    for i in range(len(X)):\n",
    "        for j in range(i+1, len(X)):\n",
    "            if X.iloc[i]['ip_address'] == X.iloc[j]['ip_address']:\n",
    "                edge_index.append([i, j])\n",
    "                edge_index.append([j, i])\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    x = torch.tensor(X.values, dtype=torch.float)\n",
    "    \n",
    "    # Check if y is already a tensor\n",
    "    if not isinstance(y, torch.Tensor):\n",
    "        y = torch.tensor(y.values, dtype=torch.long)\n",
    "    else:\n",
    "        y = y.long()\n",
    "    \n",
    "    # Make sure y has the same length as X\n",
    "    if len(y) != len(X):\n",
    "        y = y[:len(X)]\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Create graph data\n",
    "graph_data = create_graph(X, y)\n",
    "print(\"Graph created successfully\")\n",
    "print(\"Number of nodes:\", graph_data.num_nodes)\n",
    "print(\"Number of edges:\", graph_data.num_edges)\n",
    "print(\"Shape of graph_data.x:\", graph_data.x.shape)\n",
    "print(\"Shape of graph_data.y:\", graph_data.y.shape)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "graph_data.x = torch.tensor(scaler.fit_transform(graph_data.x), dtype=torch.float)\n",
    "\n",
    "# Define the Graph Autoencoder model\n",
    "class GraphAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(GraphAutoencoder, self).__init__()\n",
    "        self.encoder = GCNConv(in_channels, hidden_channels)\n",
    "        self.decoder = InnerProductDecoder()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.encoder(x, edge_index)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        return self.decoder(z, edge_index, sigmoid=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = GraphAutoencoder(in_channels=graph_data.num_features, hidden_channels=64)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model(graph_data.x, graph_data.edge_index)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    adj_pred = model.decode(z, graph_data.edge_index)\n",
    "    adj_true = torch.ones(graph_data.num_edges)\n",
    "    \n",
    "    loss = F.binary_cross_entropy(adj_pred, adj_true)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model(graph_data.x, graph_data.edge_index)\n",
    "    reconstructed_adj = model.decode(z, graph_data.edge_index)\n",
    "\n",
    "# Calculate reconstruction error for each node\n",
    "reconstruction_error = torch.zeros(graph_data.num_nodes)\n",
    "edge_index = graph_data.edge_index.t()\n",
    "for i, (u, v) in enumerate(edge_index):\n",
    "    reconstruction_error[u] += (reconstructed_adj[i] - 1) ** 2\n",
    "    reconstruction_error[v] += (reconstructed_adj[i] - 1) ** 2\n",
    "\n",
    "# Identify potential fraudulent nodes\n",
    "threshold = reconstruction_error.mean() + 2 * reconstruction_error.std()\n",
    "potential_fraud = reconstruction_error > threshold\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (potential_fraud == graph_data.y).float().mean().item()\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Get the top 5 most important features based on the encoder weights\n",
    "feature_importance = model.encoder.lin.weight.abs().mean(dim=0)\n",
    "top_features = feature_importance.argsort(descending=True)[:5]\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "print(\"\\nTop 5 most important features:\")\n",
    "for idx in top_features:\n",
    "    print(f\"{feature_names[idx.item()]}: {feature_importance[idx].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ce5953",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1000, 9)\n",
      "Shape of y: (1000,)\n",
      "Heterogeneous graph created successfully\n",
      "Epoch: 010, Loss: 18.8966, Accuracy: 0.5850\n",
      "Epoch: 020, Loss: 13.0798, Accuracy: 0.5860\n",
      "Epoch: 030, Loss: 11.4823, Accuracy: 0.4200\n",
      "Epoch: 040, Loss: 5.1393, Accuracy: 0.4200\n",
      "Epoch: 050, Loss: 13.0250, Accuracy: 0.4230\n",
      "Epoch: 060, Loss: 5.9874, Accuracy: 0.5880\n",
      "Epoch: 070, Loss: 13.6747, Accuracy: 0.5940\n",
      "Epoch: 080, Loss: 18.2725, Accuracy: 0.5920\n",
      "Epoch: 090, Loss: 6.9775, Accuracy: 0.4230\n",
      "Epoch: 100, Loss: 5.7578, Accuracy: 0.5900\n",
      "Epoch: 110, Loss: 1.4265, Accuracy: 0.5510\n",
      "Epoch: 120, Loss: 3.1669, Accuracy: 0.5920\n",
      "Epoch: 130, Loss: 8.9599, Accuracy: 0.5410\n",
      "Epoch: 140, Loss: 0.7013, Accuracy: 0.5980\n",
      "Epoch: 150, Loss: 1.5215, Accuracy: 0.4250\n",
      "Epoch: 160, Loss: 2.3310, Accuracy: 0.5910\n",
      "Epoch: 170, Loss: 10.3512, Accuracy: 0.5900\n",
      "Epoch: 180, Loss: 5.3357, Accuracy: 0.5880\n",
      "Epoch: 190, Loss: 6.7144, Accuracy: 0.5930\n",
      "Epoch: 200, Loss: 5.4863, Accuracy: 0.4290\n",
      "Epoch: 210, Loss: 7.8610, Accuracy: 0.5920\n",
      "Epoch: 220, Loss: 2.1007, Accuracy: 0.4280\n",
      "Epoch: 230, Loss: 3.0768, Accuracy: 0.5050\n",
      "Epoch: 240, Loss: 2.6084, Accuracy: 0.5990\n",
      "Epoch: 250, Loss: 3.5043, Accuracy: 0.5950\n",
      "Epoch: 260, Loss: 5.2506, Accuracy: 0.5960\n",
      "Epoch: 270, Loss: 2.4860, Accuracy: 0.5960\n",
      "Epoch: 280, Loss: 3.7903, Accuracy: 0.4310\n",
      "Epoch: 290, Loss: 4.7504, Accuracy: 0.5840\n",
      "Epoch: 300, Loss: 4.7066, Accuracy: 0.4330\n",
      "Epoch: 310, Loss: 4.6622, Accuracy: 0.4280\n",
      "Epoch: 320, Loss: 5.0510, Accuracy: 0.4280\n",
      "Epoch: 330, Loss: 2.3458, Accuracy: 0.4260\n",
      "Epoch: 340, Loss: 4.0388, Accuracy: 0.4290\n",
      "Epoch: 350, Loss: 4.7877, Accuracy: 0.4290\n",
      "Epoch: 360, Loss: 2.2896, Accuracy: 0.4270\n",
      "Epoch: 370, Loss: 3.8364, Accuracy: 0.4280\n",
      "Epoch: 380, Loss: 4.5429, Accuracy: 0.4290\n",
      "Epoch: 390, Loss: 2.3433, Accuracy: 0.4310\n",
      "Epoch: 400, Loss: 3.7112, Accuracy: 0.4310\n",
      "Epoch: 410, Loss: 4.2449, Accuracy: 0.4270\n",
      "Epoch: 420, Loss: 2.7412, Accuracy: 0.4310\n",
      "Epoch: 430, Loss: 3.6318, Accuracy: 0.4310\n",
      "Epoch: 440, Loss: 3.8184, Accuracy: 0.4310\n",
      "Epoch: 450, Loss: 3.5513, Accuracy: 0.4310\n",
      "Epoch: 460, Loss: 3.3959, Accuracy: 0.4310\n",
      "Epoch: 470, Loss: 3.3788, Accuracy: 0.4320\n",
      "Epoch: 480, Loss: 3.3643, Accuracy: 0.4320\n",
      "Epoch: 490, Loss: 3.3227, Accuracy: 0.4330\n",
      "Epoch: 500, Loss: 3.2646, Accuracy: 0.4320\n",
      "Epoch: 510, Loss: 3.2027, Accuracy: 0.4320\n",
      "Epoch: 520, Loss: 3.1442, Accuracy: 0.4310\n",
      "Epoch: 530, Loss: 3.0888, Accuracy: 0.4310\n",
      "Epoch: 540, Loss: 3.0358, Accuracy: 0.4310\n",
      "Epoch: 550, Loss: 2.9857, Accuracy: 0.4310\n",
      "Epoch: 560, Loss: 2.9375, Accuracy: 0.4310\n",
      "Epoch: 570, Loss: 2.8897, Accuracy: 0.4290\n",
      "Epoch: 580, Loss: 2.8427, Accuracy: 0.4290\n",
      "Epoch: 590, Loss: 2.7961, Accuracy: 0.4290\n",
      "Epoch: 600, Loss: 2.7508, Accuracy: 0.4300\n",
      "Epoch: 610, Loss: 2.7072, Accuracy: 0.4310\n",
      "Epoch: 620, Loss: 2.6645, Accuracy: 0.4300\n",
      "Epoch: 630, Loss: 2.6228, Accuracy: 0.4300\n",
      "Epoch: 640, Loss: 2.5821, Accuracy: 0.4300\n",
      "Epoch: 650, Loss: 2.5426, Accuracy: 0.4300\n",
      "Epoch: 660, Loss: 2.5042, Accuracy: 0.4310\n",
      "Epoch: 670, Loss: 2.4667, Accuracy: 0.4310\n",
      "Epoch: 680, Loss: 2.4301, Accuracy: 0.4310\n",
      "Epoch: 690, Loss: 2.3945, Accuracy: 0.4320\n",
      "Epoch: 700, Loss: 2.3594, Accuracy: 0.4320\n",
      "Epoch: 710, Loss: 2.3251, Accuracy: 0.4320\n",
      "Epoch: 720, Loss: 2.2921, Accuracy: 0.4320\n",
      "Epoch: 730, Loss: 2.2596, Accuracy: 0.4320\n",
      "Epoch: 740, Loss: 2.2281, Accuracy: 0.4330\n",
      "Epoch: 750, Loss: 2.1967, Accuracy: 0.4310\n",
      "Epoch: 760, Loss: 2.1668, Accuracy: 0.4310\n",
      "Epoch: 770, Loss: 2.1373, Accuracy: 0.4320\n",
      "Epoch: 780, Loss: 2.1085, Accuracy: 0.4320\n",
      "Epoch: 790, Loss: 2.0804, Accuracy: 0.4320\n",
      "Epoch: 800, Loss: 2.0530, Accuracy: 0.4320\n",
      "Epoch: 810, Loss: 2.0264, Accuracy: 0.4330\n",
      "Epoch: 820, Loss: 2.0002, Accuracy: 0.4320\n",
      "Epoch: 830, Loss: 1.9748, Accuracy: 0.4330\n",
      "Epoch: 840, Loss: 1.9491, Accuracy: 0.4360\n",
      "Epoch: 850, Loss: 1.9238, Accuracy: 0.4360\n",
      "Epoch: 860, Loss: 1.8987, Accuracy: 0.4370\n",
      "Epoch: 870, Loss: 1.8730, Accuracy: 0.4380\n",
      "Epoch: 880, Loss: 1.8475, Accuracy: 0.4380\n",
      "Epoch: 890, Loss: 1.8219, Accuracy: 0.4390\n",
      "Epoch: 900, Loss: 1.7962, Accuracy: 0.4390\n",
      "Epoch: 910, Loss: 1.7713, Accuracy: 0.4390\n",
      "Epoch: 920, Loss: 1.7478, Accuracy: 0.4390\n",
      "Epoch: 930, Loss: 1.7262, Accuracy: 0.4400\n",
      "Epoch: 940, Loss: 1.7065, Accuracy: 0.4400\n",
      "Epoch: 950, Loss: 1.6887, Accuracy: 0.4400\n",
      "Epoch: 960, Loss: 1.6726, Accuracy: 0.4410\n",
      "Epoch: 970, Loss: 1.6572, Accuracy: 0.4400\n",
      "Epoch: 980, Loss: 1.6414, Accuracy: 0.4400\n",
      "Epoch: 990, Loss: 1.6248, Accuracy: 0.4400\n",
      "Epoch: 1000, Loss: 1.6072, Accuracy: 0.4410\n",
      "Epoch: 1010, Loss: 1.5892, Accuracy: 0.4430\n",
      "Epoch: 1020, Loss: 1.5715, Accuracy: 0.4430\n",
      "Epoch: 1030, Loss: 1.5545, Accuracy: 0.4440\n",
      "Epoch: 1040, Loss: 1.5379, Accuracy: 0.4430\n",
      "Epoch: 1050, Loss: 1.5220, Accuracy: 0.4440\n",
      "Epoch: 1060, Loss: 1.5058, Accuracy: 0.4440\n",
      "Epoch: 1070, Loss: 1.4902, Accuracy: 0.4440\n",
      "Epoch: 1080, Loss: 1.4749, Accuracy: 0.4440\n",
      "Epoch: 1090, Loss: 1.4599, Accuracy: 0.4440\n",
      "Epoch: 1100, Loss: 1.4456, Accuracy: 0.4460\n",
      "Epoch: 1110, Loss: 1.4315, Accuracy: 0.4480\n",
      "Epoch: 1120, Loss: 1.4171, Accuracy: 0.4480\n",
      "Epoch: 1130, Loss: 1.4036, Accuracy: 0.4480\n",
      "Epoch: 1140, Loss: 1.3901, Accuracy: 0.4490\n",
      "Epoch: 1150, Loss: 1.3767, Accuracy: 0.4480\n",
      "Epoch: 1160, Loss: 1.3641, Accuracy: 0.4480\n",
      "Epoch: 1170, Loss: 1.3512, Accuracy: 0.4480\n",
      "Epoch: 1180, Loss: 1.3387, Accuracy: 0.4490\n",
      "Epoch: 1190, Loss: 1.3267, Accuracy: 0.4500\n",
      "Epoch: 1200, Loss: 1.3145, Accuracy: 0.4490\n",
      "Epoch: 1210, Loss: 1.3028, Accuracy: 0.4490\n",
      "Epoch: 1220, Loss: 1.2912, Accuracy: 0.4490\n",
      "Epoch: 1230, Loss: 1.2799, Accuracy: 0.4490\n",
      "Epoch: 1240, Loss: 1.2686, Accuracy: 0.4490\n",
      "Epoch: 1250, Loss: 1.2576, Accuracy: 0.4490\n",
      "Epoch: 1260, Loss: 1.2467, Accuracy: 0.4490\n",
      "Epoch: 1270, Loss: 1.2362, Accuracy: 0.4490\n",
      "Epoch: 1280, Loss: 1.2257, Accuracy: 0.4510\n",
      "Epoch: 1290, Loss: 1.2155, Accuracy: 0.4520\n",
      "Epoch: 1300, Loss: 1.2054, Accuracy: 0.4520\n",
      "Epoch: 1310, Loss: 1.1957, Accuracy: 0.4510\n",
      "Epoch: 1320, Loss: 1.1858, Accuracy: 0.4510\n",
      "Epoch: 1330, Loss: 1.1760, Accuracy: 0.4530\n",
      "Epoch: 1340, Loss: 1.1665, Accuracy: 0.4540\n",
      "Epoch: 1350, Loss: 1.1568, Accuracy: 0.4540\n",
      "Epoch: 1360, Loss: 1.1467, Accuracy: 0.4560\n",
      "Epoch: 1370, Loss: 1.1356, Accuracy: 0.4610\n",
      "Epoch: 1380, Loss: 1.1206, Accuracy: 0.4680\n",
      "Epoch: 1390, Loss: 1.0864, Accuracy: 0.4830\n",
      "Epoch: 1400, Loss: 0.8768, Accuracy: 0.6010\n",
      "Epoch: 1410, Loss: 0.7135, Accuracy: 0.6050\n",
      "Epoch: 1420, Loss: 0.6618, Accuracy: 0.5400\n",
      "Epoch: 1430, Loss: 0.6561, Accuracy: 0.6010\n",
      "Epoch: 1440, Loss: 0.6540, Accuracy: 0.6000\n",
      "Epoch: 1450, Loss: 0.6525, Accuracy: 0.6070\n",
      "Epoch: 1460, Loss: 0.6515, Accuracy: 0.6110\n",
      "Epoch: 1470, Loss: 0.6509, Accuracy: 0.6140\n",
      "Epoch: 1480, Loss: 0.6507, Accuracy: 0.6100\n",
      "Epoch: 1490, Loss: 0.6505, Accuracy: 0.6140\n",
      "Epoch: 1500, Loss: 0.6504, Accuracy: 0.6120\n",
      "Epoch: 1510, Loss: 0.6504, Accuracy: 0.6110\n",
      "Epoch: 1520, Loss: 0.6503, Accuracy: 0.6130\n",
      "Epoch: 1530, Loss: 0.6502, Accuracy: 0.6120\n",
      "Epoch: 1540, Loss: 0.6501, Accuracy: 0.6120\n",
      "Epoch: 1550, Loss: 0.6502, Accuracy: 0.6130\n",
      "Epoch: 1560, Loss: 0.6501, Accuracy: 0.6110\n",
      "Epoch: 1570, Loss: 0.6500, Accuracy: 0.6100\n",
      "Epoch: 1580, Loss: 0.6499, Accuracy: 0.6110\n",
      "Epoch: 1590, Loss: 0.6500, Accuracy: 0.6110\n",
      "Epoch: 1600, Loss: 0.6498, Accuracy: 0.6090\n",
      "Epoch: 1610, Loss: 0.6502, Accuracy: 0.6090\n",
      "Epoch: 1620, Loss: 0.6501, Accuracy: 0.6110\n",
      "Epoch: 1630, Loss: 0.6497, Accuracy: 0.6110\n",
      "Epoch: 1640, Loss: 0.6498, Accuracy: 0.6110\n",
      "Epoch: 1650, Loss: 0.6497, Accuracy: 0.6110\n",
      "Epoch: 1660, Loss: 0.6496, Accuracy: 0.6100\n",
      "Epoch: 1670, Loss: 0.6496, Accuracy: 0.6100\n",
      "Epoch: 1680, Loss: 0.6495, Accuracy: 0.6100\n",
      "Epoch: 1690, Loss: 0.6494, Accuracy: 0.6090\n",
      "Epoch: 1700, Loss: 0.6494, Accuracy: 0.6080\n",
      "Epoch: 1710, Loss: 0.6494, Accuracy: 0.6080\n",
      "Epoch: 1720, Loss: 0.6498, Accuracy: 0.6090\n",
      "Epoch: 1730, Loss: 0.6497, Accuracy: 0.6090\n",
      "Epoch: 1740, Loss: 0.6493, Accuracy: 0.6090\n",
      "Epoch: 1750, Loss: 0.6493, Accuracy: 0.6090\n",
      "Epoch: 1760, Loss: 0.6492, Accuracy: 0.6100\n",
      "Epoch: 1770, Loss: 0.6491, Accuracy: 0.6100\n",
      "Epoch: 1780, Loss: 0.6492, Accuracy: 0.6090\n",
      "Epoch: 1790, Loss: 0.6490, Accuracy: 0.6090\n",
      "Epoch: 1800, Loss: 0.6490, Accuracy: 0.6090\n",
      "Epoch: 1810, Loss: 0.6489, Accuracy: 0.6090\n",
      "Epoch: 1820, Loss: 0.6487, Accuracy: 0.6090\n",
      "Epoch: 1830, Loss: 0.6487, Accuracy: 0.6090\n",
      "Epoch: 1840, Loss: 0.6486, Accuracy: 0.6090\n",
      "Epoch: 1850, Loss: 0.6486, Accuracy: 0.6090\n",
      "Epoch: 1860, Loss: 0.6486, Accuracy: 0.6090\n",
      "Epoch: 1870, Loss: 0.6485, Accuracy: 0.6090\n",
      "Epoch: 1880, Loss: 0.6486, Accuracy: 0.6080\n",
      "Epoch: 1890, Loss: 0.6484, Accuracy: 0.6090\n",
      "Epoch: 1900, Loss: 0.6483, Accuracy: 0.6090\n",
      "Epoch: 1910, Loss: 0.6484, Accuracy: 0.6090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1920, Loss: 0.6483, Accuracy: 0.6090\n",
      "Epoch: 1930, Loss: 0.6482, Accuracy: 0.6090\n",
      "Epoch: 1940, Loss: 0.6483, Accuracy: 0.6090\n",
      "Epoch: 1950, Loss: 0.6481, Accuracy: 0.6090\n",
      "Epoch: 1960, Loss: 0.6480, Accuracy: 0.6090\n",
      "Epoch: 1970, Loss: 0.6479, Accuracy: 0.6090\n",
      "Epoch: 1980, Loss: 0.6479, Accuracy: 0.6090\n",
      "Epoch: 1990, Loss: 0.6478, Accuracy: 0.6100\n",
      "Epoch: 2000, Loss: 0.6478, Accuracy: 0.6100\n",
      "\n",
      "Final Accuracy: 0.6100\n",
      "\n",
      "Couldn't calculate feature importance. No 'user' edges found.\n",
      "\n",
      "Model structure:\n",
      "Edge type: convs\n",
      "Conv layer: ModuleDict(\n",
      "  (<user___visits___site>): SAGEConv((-1, -1), 64, aggr=mean)\n",
      "  (<user___clicks___ad>): SAGEConv((-1, -1), 64, aggr=mean)\n",
      "  (<user___self___user>): SAGEConv((-1, -1), 64, aggr=mean)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, to_hetero\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'ip_address': ip_addresses,\n",
    "    'click_time': click_time,\n",
    "    'device_type': device_type,\n",
    "    'os_version': os_version,\n",
    "    'browser': browser,\n",
    "    'site_id': site_id,\n",
    "    'ad_id': ad_id,\n",
    "    'click_count': click_count,\n",
    "    'time_on_site': time_on_site,\n",
    "    'fraud': fraud\n",
    "})\n",
    "\n",
    "# Split the data\n",
    "X = data.drop('fraud', axis=1)\n",
    "y = data['fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# Create heterogeneous graph\n",
    "def create_heterogeneous_graph(X, y):\n",
    "    data = HeteroData()\n",
    "    \n",
    "    # Add node features\n",
    "    data['user'].x = torch.tensor(X[['ip_address', 'device_type', 'os_version', 'browser']].values, dtype=torch.float)\n",
    "    data['site'].x = torch.tensor(X['site_id'].values.reshape(-1, 1), dtype=torch.float)\n",
    "    data['ad'].x = torch.tensor(X['ad_id'].values.reshape(-1, 1), dtype=torch.float)\n",
    "    \n",
    "    # Add edges\n",
    "    num_nodes = len(X)\n",
    "    edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t()\n",
    "    data['user', 'visits', 'site'].edge_index = edge_index\n",
    "    data['user', 'clicks', 'ad'].edge_index = edge_index\n",
    "    \n",
    "    # Add self-loops for user nodes\n",
    "    user_self_loops = torch.arange(num_nodes).repeat(2, 1)\n",
    "    data['user', 'self', 'user'].edge_index = user_self_loops\n",
    "    \n",
    "    # Add edge features\n",
    "    data['user', 'visits', 'site'].edge_attr = torch.tensor(X['time_on_site'].values.reshape(-1, 1), dtype=torch.float)\n",
    "    data['user', 'clicks', 'ad'].edge_attr = torch.tensor(X['click_count'].values.reshape(-1, 1), dtype=torch.float)\n",
    "    \n",
    "    # Add labels\n",
    "    data['user'].y = torch.tensor(y.values, dtype=torch.long)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Create heterogeneous graph\n",
    "graph = create_heterogeneous_graph(X, y)\n",
    "print(\"Heterogeneous graph created successfully\")\n",
    "\n",
    "# Define the Heterogeneous GNN model\n",
    "class HGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, metadata):\n",
    "        super().__init__()\n",
    "        self.conv1 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_channels)\n",
    "            for edge_type in metadata[1]\n",
    "        })\n",
    "        self.conv2 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), out_channels)\n",
    "            for edge_type in metadata[1]\n",
    "        })\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        return x_dict\n",
    "\n",
    "# Initialize the model\n",
    "model = HGNN(hidden_channels=64, out_channels=2, metadata=graph.metadata())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(graph.x_dict, graph.edge_index_dict)\n",
    "    loss = F.cross_entropy(out['user'], graph['user'].y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(graph.x_dict, graph.edge_index_dict)\n",
    "    pred = out['user'].argmax(dim=-1)\n",
    "    accuracy = (pred == graph['user'].y).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1, 2001):\n",
    "    loss = train()\n",
    "    if epoch % 10 == 0:\n",
    "        accuracy = test()\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "# Final evaluation\n",
    "final_accuracy = test()\n",
    "print(f'\\nFinal Accuracy: {final_accuracy:.4f}')\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = None\n",
    "for edge_type, conv in model.conv1._modules.items():\n",
    "    if edge_type[0] == 'user':  # We're interested in edges starting from 'user'\n",
    "        if feature_importance is None:\n",
    "            feature_importance = conv.lin_l.weight.abs().mean(dim=0)\n",
    "        else:\n",
    "            feature_importance += conv.lin_l.weight.abs().mean(dim=0)\n",
    "\n",
    "if feature_importance is not None:\n",
    "    # Normalize feature importance\n",
    "    feature_importance /= len(model.conv1._modules)\n",
    "\n",
    "    # Get top 4 important features (since we have 4 input features for users)\n",
    "    top_features = feature_importance.argsort(descending=True)\n",
    "    feature_names = ['IP', 'Device', 'OS', 'Browser']\n",
    "\n",
    "    print(\"\\nFeature importance:\")\n",
    "    for idx in range(4):\n",
    "        print(f\"{feature_names[idx]}: {feature_importance[idx].item():.4f}\")\n",
    "else:\n",
    "    print(\"\\nCouldn't calculate feature importance. No 'user' edges found.\")\n",
    "\n",
    "# Print model structure\n",
    "print(\"\\nModel structure:\")\n",
    "for edge_type, conv in model.conv1._modules.items():\n",
    "    print(f\"Edge type: {edge_type}\")\n",
    "    print(f\"Conv layer: {conv}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22df7ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM Results:\n",
      "Accuracy: 0.5862\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Prepare sequential data\n",
    "seq_length = 5\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(len(X_train) - seq_length):\n",
    "    X_seq.append(X_train_scaled[i:i+seq_length])\n",
    "    y_seq.append(y_train.iloc[i+seq_length])\n",
    "\n",
    "X_seq = torch.tensor(np.array(X_seq), dtype=torch.float32)\n",
    "y_seq = torch.tensor(np.array(y_seq), dtype=torch.long)\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = LSTMModel(input_size=X_train.shape[1], hidden_size=64, num_layers=2, num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_seq)\n",
    "    loss = criterion(outputs, y_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_seq)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    acc = (predicted == y_seq).sum().item() / y_seq.size(0)\n",
    "\n",
    "print(\"\\nLSTM Results:\")\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "628f9a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5043 - loss: 0.7035 - val_accuracy: 0.6187 - val_loss: 0.6705\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6020 - loss: 0.6712 - val_accuracy: 0.6187 - val_loss: 0.6710\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5484 - loss: 0.6938 - val_accuracy: 0.6187 - val_loss: 0.6744\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5892 - loss: 0.6821 - val_accuracy: 0.6187 - val_loss: 0.6725\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5592 - loss: 0.6901 - val_accuracy: 0.6187 - val_loss: 0.6701\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5893 - loss: 0.6796 - val_accuracy: 0.6187 - val_loss: 0.6659\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5804 - loss: 0.6769 - val_accuracy: 0.6187 - val_loss: 0.6693\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5639 - loss: 0.6823 - val_accuracy: 0.6187 - val_loss: 0.6689\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5773 - loss: 0.6814 - val_accuracy: 0.6187 - val_loss: 0.6710\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5890 - loss: 0.6748 - val_accuracy: 0.6187 - val_loss: 0.6686\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "\n",
      "1D CNN Results:\n",
      "Accuracy: 0.5800\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "# Split the data\n",
    "X = data.drop('fraud', axis=1)\n",
    "y = data['fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for Conv1D\n",
    "X_train_scaled = np.expand_dims(X_train_scaled, axis=2)\n",
    "X_test_scaled = np.expand_dims(X_test_scaled, axis=2)\n",
    "\n",
    "# Build 1D CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    Conv1D(filters=64, kernel_size=2, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "# Predict and calculate accuracy manually\n",
    "y_pred = (model.predict(X_test_scaled) > 0.5).astype(int).flatten()\n",
    "acc = (y_pred == y_test.to_numpy()).sum() / y_test.size\n",
    "\n",
    "print(\"\\n1D CNN Results:\")\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a4aebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.3217, Val Loss: 0.3433\n",
      "Epoch [20/100], Loss: 0.0670, Val Loss: 0.0844\n",
      "Epoch [30/100], Loss: 0.0110, Val Loss: 0.0149\n",
      "Epoch [40/100], Loss: 0.0082, Val Loss: 0.0089\n",
      "Epoch [50/100], Loss: 0.0069, Val Loss: 0.0079\n",
      "Epoch [60/100], Loss: 0.0063, Val Loss: 0.0074\n",
      "Epoch [70/100], Loss: 0.0091, Val Loss: 0.0073\n",
      "Epoch [80/100], Loss: 0.0075, Val Loss: 0.0099\n",
      "Epoch [90/100], Loss: 0.0084, Val Loss: 0.0124\n",
      "Epoch [100/100], Loss: 0.0088, Val Loss: 0.0142\n",
      "\n",
      "Autoencoder Results:\n",
      "Accuracy: 0.5800\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "# Split training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "\n",
    "model = Autoencoder(input_dim=X_train.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        batch = X_train_tensor[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, X_val_tensor)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed_train = model(X_train_tensor)\n",
    "    reconstructed_test = model(X_test_tensor)\n",
    "    \n",
    "    mse = nn.MSELoss(reduction='none')\n",
    "    mse_loss_train = mse(reconstructed_train, X_train_tensor).mean(axis=1)\n",
    "    mse_loss_test = mse(reconstructed_test, X_test_tensor).mean(axis=1)\n",
    "    \n",
    "    # Normalize the reconstruction error\n",
    "    mse_loss_train = (mse_loss_train - mse_loss_train.mean()) / mse_loss_train.std()\n",
    "    mse_loss_test = (mse_loss_test - mse_loss_train.mean()) / mse_loss_train.std()\n",
    "    \n",
    "    # Use a percentile for thresholding\n",
    "    threshold = np.percentile(mse_loss_train.numpy(), 95)\n",
    "    \n",
    "    predictions = (mse_loss_test > threshold).int()\n",
    "    \n",
    "    # Convert y_test to a numpy array, then to a torch tensor\n",
    "    y_test_tensor = torch.tensor(y_test.values)\n",
    "    \n",
    "    acc = (predictions == y_test_tensor).float().mean().item()\n",
    "\n",
    "print(\"\\nAutoencoder Results:\")\n",
    "print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df19dd83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], D Loss: 0.8170, G Loss: 1.0999\n",
      "Epoch [20/200], D Loss: 0.5701, G Loss: 2.4183\n",
      "Epoch [30/200], D Loss: 0.2808, G Loss: 2.5043\n",
      "Epoch [40/200], D Loss: 0.2979, G Loss: 2.2014\n",
      "Epoch [50/200], D Loss: 0.8416, G Loss: 3.6320\n",
      "Epoch [60/200], D Loss: 0.1717, G Loss: 3.5408\n",
      "Epoch [70/200], D Loss: 0.3524, G Loss: 1.8411\n",
      "Epoch [80/200], D Loss: 0.1820, G Loss: 2.7393\n",
      "Epoch [90/200], D Loss: 0.1959, G Loss: 3.0018\n",
      "Epoch [100/200], D Loss: 0.3268, G Loss: 2.6795\n",
      "Epoch [110/200], D Loss: 0.3382, G Loss: 2.7643\n",
      "Epoch [120/200], D Loss: 0.1907, G Loss: 3.1943\n",
      "Epoch [130/200], D Loss: 0.3630, G Loss: 2.5350\n",
      "Epoch [140/200], D Loss: 0.2917, G Loss: 3.0537\n",
      "Epoch [150/200], D Loss: 0.1255, G Loss: 2.9887\n",
      "Epoch [160/200], D Loss: 0.4486, G Loss: 2.6179\n",
      "Epoch [170/200], D Loss: 0.3201, G Loss: 2.8845\n",
      "Epoch [180/200], D Loss: 0.5896, G Loss: 2.5527\n",
      "Epoch [190/200], D Loss: 0.1774, G Loss: 2.8180\n",
      "Epoch [200/200], D Loss: 0.4390, G Loss: 2.4391\n",
      "Original data shape: (1000, 10)\n",
      "Augmented data shape: (2000, 10)\n",
      "Percentage of fraud samples in augmented dataset: 70.70%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'ip_address': ip_addresses,\n",
    "    'click_time': click_time,\n",
    "    'device_type': device_type,\n",
    "    'os_version': os_version,\n",
    "    'browser': browser,\n",
    "    'site_id': site_id,\n",
    "    'ad_id': ad_id,\n",
    "    'click_count': click_count,\n",
    "    'time_on_site': time_on_site,\n",
    "    'fraud': fraud\n",
    "})\n",
    "\n",
    "# Split the data\n",
    "X = data.drop('fraud', axis=1)\n",
    "y = data['fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "num_epochs = 200\n",
    "lr = 0.0002\n",
    "\n",
    "# Initialize Generator and Discriminator\n",
    "generator = Generator(latent_dim, input_dim)\n",
    "discriminator = Discriminator(input_dim)\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_samples, _) in enumerate(train_loader):\n",
    "        batch_size = real_samples.size(0)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Real samples\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        outputs = discriminator(real_samples)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Fake samples\n",
    "        noise = torch.randn(batch_size, latent_dim)\n",
    "        fake_samples = generator(noise)\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "        outputs = discriminator(fake_samples.detach())\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        \n",
    "        # Total Discriminator loss\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        outputs = discriminator(fake_samples)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "# Generate synthetic fraud samples\n",
    "num_synthetic_samples = 1000\n",
    "noise = torch.randn(num_synthetic_samples, latent_dim)\n",
    "synthetic_samples = generator(noise).detach().numpy()\n",
    "\n",
    "# Scale back the synthetic samples\n",
    "synthetic_samples_original = scaler.inverse_transform(synthetic_samples)\n",
    "\n",
    "# Create a DataFrame with synthetic samples\n",
    "synthetic_df = pd.DataFrame(synthetic_samples_original, columns=X.columns)\n",
    "synthetic_df['fraud'] = 1  # Label all synthetic samples as fraud\n",
    "\n",
    "# Combine original and synthetic data\n",
    "augmented_data = pd.concat([data, synthetic_df], ignore_index=True)\n",
    "\n",
    "print(\"Original data shape:\", data.shape)\n",
    "print(\"Augmented data shape:\", augmented_data.shape)\n",
    "\n",
    "# Calculate the percentage of fraud samples in the augmented dataset\n",
    "fraud_percentage = (augmented_data['fraud'] == 1).mean() * 100\n",
    "print(f\"Percentage of fraud samples in augmented dataset: {fraud_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "779d06d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.0080, Accuracy: 0.9950\n",
      "Epoch [20/100], Loss: 0.0036, Accuracy: 0.9900\n",
      "Epoch [30/100], Loss: 0.0009, Accuracy: 0.9950\n",
      "Epoch [40/100], Loss: 0.0005, Accuracy: 0.9950\n",
      "Epoch [50/100], Loss: 0.0006, Accuracy: 0.9950\n",
      "Epoch [60/100], Loss: 0.0003, Accuracy: 0.9950\n",
      "Epoch [70/100], Loss: 0.0002, Accuracy: 0.9950\n",
      "Epoch [80/100], Loss: 0.0002, Accuracy: 0.9950\n",
      "Epoch [90/100], Loss: 0.0001, Accuracy: 0.9950\n",
      "Epoch [100/100], Loss: 0.0001, Accuracy: 0.9950\n",
      "\n",
      "Final Test Accuracy: 0.9950\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       116\n",
      "           1       1.00      0.99      0.99        84\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       1.00      0.99      0.99       200\n",
      "weighted avg       1.00      0.99      0.99       200\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[116   0]\n",
      " [  1  83]]\n",
      "\n",
      "Feature Importance:\n",
      "click_count: 0.0346\n",
      "ad_id: 0.0340\n",
      "device_type: 0.0314\n",
      "time_on_site: 0.0313\n",
      "site_id: 0.0297\n",
      "os_version: 0.0285\n",
      "browser: 0.0281\n",
      "click_time: 0.0255\n",
      "ip_address: 0.0253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'ip_address': ip_addresses,\n",
    "    'click_time': click_time,\n",
    "    'device_type': device_type,\n",
    "    'os_version': os_version,\n",
    "    'browser': browser,\n",
    "    'site_id': site_id,\n",
    "    'ad_id': ad_id,\n",
    "    'click_count': click_count,\n",
    "    'time_on_site': time_on_site,\n",
    "    'fraud': fraud\n",
    "})\n",
    "\n",
    "# Split the data\n",
    "X = data.drop('fraud', axis=1)\n",
    "y = data['fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.LongTensor(y_train.values)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.LongTensor(y_test.values)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dim_model=32, num_heads=2, num_layers=1, dropout=0.1):\n",
    "        super(TabularTransformer, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(input_dim, dim_model)\n",
    "        self.pos_encoder = nn.Linear(1, dim_model)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=dim_model, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(dim_model, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add a sequence dimension\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        pos = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1).unsqueeze(-1).to(x.device)\n",
    "        pos = self.pos_encoder(pos.float())\n",
    "        x = x + pos\n",
    "        \n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        x = x.squeeze(1)  # Remove the sequence dimension\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "num_classes = 2\n",
    "dim_model = 32\n",
    "num_heads = 2\n",
    "num_layers = 1\n",
    "dropout = 0.1\n",
    "num_epochs = 100\n",
    "lr = 0.001\n",
    "\n",
    "# Initialize the model\n",
    "model = TabularTransformer(input_dim, num_classes, dim_model, num_heads, num_layers, dropout)\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        y_true.extend(batch_y.numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "final_accuracy = correct / total\n",
    "print(f\"\\nFinal Test Accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Feature importance (using embedding weights)\n",
    "def get_feature_importance(model, feature_names):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get the weights of the embedding layer\n",
    "        embedding_weights = model.embedding.weight.abs().mean(dim=1).numpy()\n",
    "        \n",
    "        # Normalize the weights\n",
    "        embedding_weights = embedding_weights / np.sum(embedding_weights)\n",
    "        \n",
    "        # Create a dictionary of feature names and their importance\n",
    "        feature_importance = dict(zip(feature_names, embedding_weights))\n",
    "        \n",
    "        # Sort the features by importance\n",
    "        sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "    return sorted_features\n",
    "\n",
    "# Get feature importance\n",
    "feature_names = X.columns\n",
    "feature_importance = get_feature_importance(model, feature_names)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "for name, importance in feature_importance:\n",
    "    print(f\"{name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d04f0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wr/j4rtgdp95y39kz0h4jbflgvm0000gn/T/ipykernel_24851/1704990393.py:78: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  states = torch.FloatTensor(states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Total Reward: 582, Accuracy: 0.8638\n",
      "Epoch 10, Total Reward: 790, Accuracy: 0.9938\n",
      "Epoch 20, Total Reward: 796, Accuracy: 0.9975\n",
      "Epoch 30, Total Reward: 794, Accuracy: 0.9962\n",
      "Epoch 40, Total Reward: 792, Accuracy: 0.9950\n",
      "Epoch 50, Total Reward: 798, Accuracy: 0.9988\n",
      "Epoch 60, Total Reward: 796, Accuracy: 0.9975\n",
      "Epoch 70, Total Reward: 796, Accuracy: 0.9975\n",
      "Epoch 80, Total Reward: 798, Accuracy: 0.9988\n",
      "Epoch 90, Total Reward: 798, Accuracy: 0.9988\n",
      "Test Accuracy: 0.9900\n",
      "\n",
      "Feature Importance:\n",
      "click_count: 0.3623\n",
      "device_type: 0.2888\n",
      "site_id: 0.2791\n",
      "time_on_site: 0.2722\n",
      "os_version: 0.2428\n",
      "ip_address: 0.2422\n",
      "browser: 0.2371\n",
      "ad_id: 0.2348\n",
      "click_time: 0.2051\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'ip_address': ip_addresses,\n",
    "    'click_time': click_time,\n",
    "    'device_type': device_type,\n",
    "    'os_version': os_version,\n",
    "    'browser': browser,\n",
    "    'site_id': site_id,\n",
    "    'ad_id': ad_id,\n",
    "    'click_count': click_count,\n",
    "    'time_on_site': time_on_site,\n",
    "    'fraud': fraud\n",
    "})\n",
    "\n",
    "# Split the data\n",
    "X = data.drop('fraud', axis=1)\n",
    "y = data['fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the DQN model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Define the DRL agent\n",
    "class DRLAgent:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.model = DQN(state_dim, action_dim)\n",
    "        self.target_model = DQN(state_dim, action_dim)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = self.model(state)\n",
    "        return q_values.argmax().item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.target_model(next_states).max(1)[0]\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "# Training function\n",
    "def train_drl_agent(agent, X_train, y_train, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        total_reward = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for i in range(len(X_train)):\n",
    "            state = X_train[i]\n",
    "            action = agent.act(state)\n",
    "            true_label = y_train.iloc[i]\n",
    "\n",
    "            reward = 1 if action == true_label else -1\n",
    "            total_reward += reward\n",
    "            correct_predictions += int(action == true_label)\n",
    "\n",
    "            done = (i == len(X_train) - 1)\n",
    "            next_state = X_train[i+1] if not done else np.zeros_like(state)\n",
    "\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.replay()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            agent.update_target_model()\n",
    "            accuracy = correct_predictions / len(X_train)\n",
    "            print(f\"Epoch {epoch}, Total Reward: {total_reward}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_drl_agent(agent, X_test, y_test):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(X_test)):\n",
    "        state = X_test[i]\n",
    "        action = agent.act(state)\n",
    "        true_label = y_test.iloc[i]\n",
    "        correct_predictions += int(action == true_label)\n",
    "    \n",
    "    accuracy = correct_predictions / len(X_test)\n",
    "    return accuracy\n",
    "\n",
    "# Initialize and train the DRL agent\n",
    "state_dim = X_train_scaled.shape[1]\n",
    "action_dim = 2  # 0 for non-fraud, 1 for fraud\n",
    "agent = DRLAgent(state_dim, action_dim)\n",
    "\n",
    "train_drl_agent(agent, X_train_scaled, y_train, epochs=100)\n",
    "\n",
    "# Evaluate the agent\n",
    "test_accuracy = evaluate_drl_agent(agent, X_test_scaled, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = agent.model.fc1.weight.abs().mean(dim=0).detach().numpy()\n",
    "feature_names = X.columns\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "for name, importance in sorted(zip(feature_names, feature_importance), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13f97b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f59f1cdf57e498cbd040955d9a44311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/1679 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 3): 100%|██████████| 50/50 [00:03<00:00, 15.83it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 50/50 [00:03<00:00, 15.79it/s]\n",
      "Generating walks (CPU: 4): 100%|██████████| 50/50 [00:03<00:00, 15.66it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 50/50 [00:03<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       116\n",
      "           1       0.99      0.99      0.99        84\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       0.99      0.99      0.99       200\n",
      "weighted avg       0.99      0.99      0.99       200\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[115   1]\n",
      " [  1  83]]\n",
      "click_count: 1.0000\n",
      "ip_address: 0.0000\n",
      "click_time: 0.0000\n",
      "device_type: 0.0000\n",
      "os_version: 0.0000\n",
      "browser: 0.0000\n",
      "site_id: 0.0000\n",
      "ad_id: 0.0000\n",
      "time_on_site: 0.0000\n",
      "ip_degree: 0.0000\n",
      "site_degree: 0.0000\n",
      "ad_degree: 0.0000\n",
      "ip_embedding: 0.0000\n",
      "site_embedding: 0.0000\n",
      "ad_embedding: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'ip_address': ip_addresses,\n",
    "    'click_time': click_time,\n",
    "    'device_type': device_type,\n",
    "    'os_version': os_version,\n",
    "    'browser': browser,\n",
    "    'site_id': site_id,\n",
    "    'ad_id': ad_id,\n",
    "    'click_count': click_count,\n",
    "    'time_on_site': time_on_site,\n",
    "    'fraud': fraud\n",
    "})\n",
    "\n",
    "# Split the data\n",
    "X = data.drop('fraud', axis=1)\n",
    "y = data['fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 1: Create a graph from the data and extract additional features\n",
    "def create_graph_and_features(data):\n",
    "    G = nx.Graph()\n",
    "    for _, row in data.iterrows():\n",
    "        G.add_node(f\"ip_{row['ip_address']}\", type='ip')\n",
    "        G.add_node(f\"site_{row['site_id']}\", type='site')\n",
    "        G.add_node(f\"ad_{row['ad_id']}\", type='ad')\n",
    "        G.add_edge(f\"ip_{row['ip_address']}\", f\"site_{row['site_id']}\")\n",
    "        G.add_edge(f\"ip_{row['ip_address']}\", f\"ad_{row['ad_id']}\")\n",
    "    \n",
    "    # Extract additional features\n",
    "    ip_degree = {node: deg for node, deg in G.degree() if node.startswith('ip_')}\n",
    "    site_degree = {node: deg for node, deg in G.degree() if node.startswith('site_')}\n",
    "    ad_degree = {node: deg for node, deg in G.degree() if node.startswith('ad_')}\n",
    "    \n",
    "    data['ip_degree'] = data['ip_address'].apply(lambda x: ip_degree.get(f\"ip_{x}\", 0))\n",
    "    data['site_degree'] = data['site_id'].apply(lambda x: site_degree.get(f\"site_{x}\", 0))\n",
    "    data['ad_degree'] = data['ad_id'].apply(lambda x: ad_degree.get(f\"ad_{x}\", 0))\n",
    "    \n",
    "    return G, data\n",
    "\n",
    "# Step 2: Generate graph embeddings using node2vec\n",
    "def generate_embeddings(G):\n",
    "    node2vec = Node2Vec(G, dimensions=128, walk_length=30, num_walks=200, workers=4)\n",
    "    model = node2vec.fit(window=10, min_count=1)\n",
    "    return model\n",
    "\n",
    "# Step 3: Create a deep neural network\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Step 4: Combine features and train models\n",
    "def train_models(X_train, y_train, embeddings):\n",
    "    # Combine original features with graph embeddings\n",
    "    X_train_combined = []\n",
    "    for _, row in X_train.iterrows():\n",
    "        features = row.values\n",
    "        ip_embedding = embeddings.wv[f\"ip_{row['ip_address']}\"]\n",
    "        site_embedding = embeddings.wv[f\"site_{row['site_id']}\"]\n",
    "        ad_embedding = embeddings.wv[f\"ad_{row['ad_id']}\"]\n",
    "        combined = np.concatenate([features, ip_embedding, site_embedding, ad_embedding])\n",
    "        X_train_combined.append(combined)\n",
    "    X_train_combined = np.array(X_train_combined)\n",
    "    \n",
    "    # Train deep neural network\n",
    "    dnn = DeepNN(X_train_combined.shape[1])\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(dnn.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    \n",
    "    for epoch in range(200):  # Increased number of epochs\n",
    "        optimizer.zero_grad()\n",
    "        outputs = dnn(torch.FloatTensor(X_train_combined))\n",
    "        loss = criterion(outputs.squeeze(), torch.FloatTensor(y_train.values))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Train gradient boosting classifier with hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    gbc = RandomizedSearchCV(GradientBoostingClassifier(), param_grid, n_iter=10, cv=3, n_jobs=-1)\n",
    "    gbc.fit(X_train_combined, y_train)\n",
    "    \n",
    "    # Train random forest classifier\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train_combined, y_train)\n",
    "    \n",
    "    return dnn, gbc.best_estimator_, rf\n",
    "\n",
    "# Step 5: Make predictions using ensemble\n",
    "def predict_ensemble(X_test, dnn, gbc, rf, embeddings):\n",
    "    X_test_combined = []\n",
    "    for _, row in X_test.iterrows():\n",
    "        features = row.values\n",
    "        ip_embedding = embeddings.wv[f\"ip_{row['ip_address']}\"]\n",
    "        site_embedding = embeddings.wv[f\"site_{row['site_id']}\"]\n",
    "        ad_embedding = embeddings.wv[f\"ad_{row['ad_id']}\"]\n",
    "        combined = np.concatenate([features, ip_embedding, site_embedding, ad_embedding])\n",
    "        X_test_combined.append(combined)\n",
    "    X_test_combined = np.array(X_test_combined)\n",
    "    \n",
    "    dnn_pred = dnn(torch.FloatTensor(X_test_combined)).detach().numpy()\n",
    "    gbc_pred = gbc.predict_proba(X_test_combined)[:, 1]\n",
    "    rf_pred = rf.predict_proba(X_test_combined)[:, 1]\n",
    "    \n",
    "    ensemble_pred = (dnn_pred.squeeze() + gbc_pred + rf_pred) / 3\n",
    "    return (ensemble_pred > 0.5).astype(int)\n",
    "\n",
    "# Main execution\n",
    "G, data = create_graph_and_features(data)\n",
    "X = data.drop('fraud', axis=1)\n",
    "y = data['fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "embeddings = generate_embeddings(G)\n",
    "dnn, gbc, rf = train_models(X_train, y_train, embeddings)\n",
    "y_pred = predict_ensemble(X_test, dnn, gbc, rf, embeddings)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Feature importance (from GradientBoostingClassifier)\n",
    "feature_importance = gbc.feature_importances_\n",
    "feature_names = list(X.columns) + ['ip_embedding', 'site_embedding', 'ad_embedding']\n",
    "for name, importance in sorted(zip(feature_names, feature_importance), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66340df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
